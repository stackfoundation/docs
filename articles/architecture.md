# Architecture

Let's take a look the architecture of Sandbox by breaking it down into the various components that make it up:

- `sbox`: When you run Sandbox on Unix, you begin by running the `sbox` Unix script at the root of your project. It's only purpose is to invoke one of the binaries inside `sbox-cli`, based on your current OS and platform.
- `sbox.exe`, `wrapper-x.y.z-darwin-386`, `wrapper-x.y.z-linux-386`, `wrapper-x.y.z-linux-amd64`: These are referred to as the "wrapper" binaries. The wrapper binaries have a very simple purpose: on first run, they download and cache the larger "bootstrap" binaries from a HTTP server. On all subsequent runs, the wrapper binaries then simply call the bootstrap binaries with whatever arguments are passed in. They are written in C++ and are built to be tiny in size so that you can commit them in to your Git repository without worrying about size. Because they are intended to be tiny, they are limited in functionality - most importantly, they can only download from a HTTP server, and not a HTTPS server (SSL/TLS adds a lot of code weight). Because they donwload bootstrap binaries from a HTTP server, for security reasons, the wrapper binaries also perform a SHA-256 hash of the binaries that are downloaded and verify their signature before launching the bootstrap binaries. This verification is done on every run.
- `bootstrap-x.y.z-[os]-[arch][.exe]`: Referred to as "bootstrap" binaries, these binaries are downloaded and cached by the wrapper binaries. The bootstrap binaries also have a very simple purpose: on first run, they download and cache the much larger "core" binaries from a HTTPS server. On all subsequent runs, the bootstrap binaries then simply call the core binaries with whatever arguments are passed in. These are written in Go and have enough functionality to download the core binaries tar'ed and gzip'ed from a HTTPS server and decompress them.
- `core-x.y.z-[os]-[arch][.exe]`: Referred to as "core" binaries, these binaries are downloaded and cached by the bootstrap binaries. These make up the bulk of Sandbox. The Sandbox core is built on top of the Kubernetes [minikube](https://github.com/kubernetes/minikube) project, and is [open source](https://github.com/stackfoundation/sandbox). Because it is built on minikube, the architecture used by Sandbox core mirrors what you would see in minikube:
    - **VM**: A linux-kernel based image with a Docker daemon is run on a virtual machine, and serves as the host for the single-node Kubernetes cluster that is started by Sandbox core. The VM is run using whatever hypervisor framework is available on the platform. This may be HyperV on certain Windows versions, or the Hypervisor framework on macOS. Sandbox downloads and installs a version of [VirtualBox](https://www.virtualbox.org/) in other cases, and uses VirtualBox to run the VM. Note that the VM image and much of the provisioning around it is borrowed from minikube, which in turn uses [Docker Machine](https://github.com/docker/machine) and it's various drivers. That means many of the customization options available in minikube around provisioning the VM are available in Sandbox.
    - `localkube`: This is a single Go process that runs many core components of Kubernetes (such as the API server, controller manager, etc.) that would otherwise be started as separate processes. This process runs within the VM that is started by Sandbox.
    - **Workflow engine**: The heart of Sandbox itself is a workflow engine which reads the workflow `.yml` files and translates them into various Kubernetes constructs. For example, many `run` steps are translated into pod executions, and the `service` steps result in the creation of Kubernetes services. The details of how workflow constructs are translated into Kubernets constructs is an implementation detail but the [workflows reference](/docs/workflows) should offer some glimpses.
    - **CLI**: The Sandbox CLI borrows much of the provisioning process for setting up the VM and `localkube` from minikube. Once the VM is provisioned, the Sandbox CLI connects to the Kubernets API server, and the Docker daemon, and uses the workflow engine to run workflows.

